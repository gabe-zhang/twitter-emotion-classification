{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Model Evaluation\n",
    "\n",
    "This notebook evaluates trained models:\n",
    "- Load saved model checkpoint\n",
    "- Evaluate on test set\n",
    "- Analyze predictions and errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from src.config import EMOTION_LABELS\n",
    "from src.data.dataset import load_emotion_data, get_tokenizer\n",
    "from src.models.bert_classifier import BertClassifier\n",
    "from src.training.trainer import Trainer\n",
    "from src.training.utils import load_checkpoint, get_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "\n",
    "# Initialize and load model\n",
    "model = BertClassifier()\n",
    "model = load_checkpoint(model, filename='best_model.pth', device=device)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "_, _, test_df = load_emotion_data(resample=False)\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "# Create trainer for evaluation\n",
    "trainer = Trainer(model=model)\n",
    "results = trainer.evaluate(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Accuracy: {results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Score: {results['f1_score']:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(results['classification_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for confusion matrix\n",
    "import torch\n",
    "from src.data.dataset import EmotionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "test_dataset = EmotionDataset(test_df, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4)\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch_input, batch_label in test_loader:\n",
    "        mask = batch_input['attention_mask'].to(device)\n",
    "        input_id = batch_input['input_ids'].squeeze(1).to(device)\n",
    "        output = model(input_id, mask)\n",
    "        all_preds.extend(output.argmax(dim=1).cpu().numpy())\n",
    "        all_labels.extend(batch_label.numpy())\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=list(EMOTION_LABELS.values()),\n",
    "    yticklabels=list(EMOTION_LABELS.values())\n",
    ")\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prediction\n",
    "from src.config import MAX_LENGTH\n",
    "\n",
    "def predict_emotion(text):\n",
    "    \"\"\"Predict emotion for a single text.\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        output = model(input_ids, attention_mask)\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        pred_class = output.argmax(dim=1).item()\n",
    "        confidence = probs[0][pred_class].item()\n",
    "    \n",
    "    return EMOTION_LABELS[pred_class], confidence\n",
    "\n",
    "# Test predictions\n",
    "test_texts = [\n",
    "    \"I just got promoted at work! Best day ever!\",\n",
    "    \"I can't believe they would do this to me.\",\n",
    "    \"Missing my grandmother who passed away last year.\",\n",
    "    \"The test results came back... I'm so scared.\",\n",
    "    \"You're the best thing that ever happened to me.\",\n",
    "    \"Wow, I never expected that ending!\"\n",
    "]\n",
    "\n",
    "print(\"Sample Predictions:\\n\")\n",
    "for text in test_texts:\n",
    "    emotion, conf = predict_emotion(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"  â†’ {emotion} ({conf:.1%} confidence)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples\n",
    "test_df_copy = test_df.copy()\n",
    "test_df_copy['predicted'] = [EMOTION_LABELS[p] for p in all_preds]\n",
    "test_df_copy['actual'] = test_df_copy['category']\n",
    "test_df_copy['correct'] = test_df_copy['predicted'] == test_df_copy['actual']\n",
    "\n",
    "# Show some misclassified examples\n",
    "misclassified = test_df_copy[~test_df_copy['correct']].sample(min(10, len(test_df_copy[~test_df_copy['correct']])))\n",
    "print(\"Sample Misclassified Examples:\\n\")\n",
    "for _, row in misclassified.iterrows():\n",
    "    print(f\"Text: {row['text'][:100]}...\")\n",
    "    print(f\"  Actual: {row['actual']} | Predicted: {row['predicted']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
