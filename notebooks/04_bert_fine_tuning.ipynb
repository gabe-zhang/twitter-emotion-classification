{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - BERT Fine-Tuning\n",
    "\n",
    "This notebook demonstrates fine-tuning BERT for emotion classification:\n",
    "- Using pre-trained emotion BERT models\n",
    "- Fine-tuning bert-base-cased on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "from src.config import EMOTION_LABELS, MODEL_NAME\n",
    "from src.data.dataset import load_emotion_data, get_tokenizer\n",
    "from src.models.bert_classifier import BertClassifier\n",
    "from src.training.trainer import Trainer\n",
    "from src.training.utils import get_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pre-trained Emotion BERT\n",
    "\n",
    "First, let's try a model already fine-tuned for emotion classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained emotion classifier from HuggingFace\n",
    "emotion_classifier = pipeline(\n",
    "    'text-classification',\n",
    "    model='bhadresh-savani/bert-base-uncased-emotion',\n",
    "    return_all_scores=True\n",
    ")\n",
    "\n",
    "# Test on sample texts\n",
    "test_texts = [\n",
    "    \"I am so happy today!\",\n",
    "    \"This makes me really angry.\",\n",
    "    \"I feel sad and lonely.\",\n",
    "    \"What a wonderful surprise!\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = emotion_classifier(text)\n",
    "    top_emotion = max(result[0], key=lambda x: x['score'])\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"  Emotion: {top_emotion['label']} ({top_emotion['score']:.2%})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Our Own BERT Model\n",
    "\n",
    "Now let's fine-tune bert-base-cased on our specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check device\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load data with resampling\n",
    "train_df, val_df, test_df = load_emotion_data(resample=True)\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train: {len(train_df)} (resampled)\")\n",
    "print(f\"  Val: {len(val_df)}\")\n",
    "print(f\"  Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = BertClassifier()\n",
    "print(f\"Model architecture:\\n{model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "# Note: For notebook demo, using fewer epochs. Use scripts/train.py for full training.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    learning_rate=5e-7,\n",
    "    batch_size=4,\n",
    "    epochs=2,  # Reduced for demo - use 20 for full training\n",
    "    use_data_parallel=False\n",
    ")\n",
    "\n",
    "print(\"Starting training (demo with 2 epochs)...\")\n",
    "print(\"For full training, run: python scripts/train.py --epochs 20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model (uncomment to run - takes a while)\n",
    "# trainer.train(train_df, val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture Details\n",
    "\n",
    "Our BertClassifier architecture:\n",
    "\n",
    "```\n",
    "BERT Encoder (bert-base-cased)\n",
    "    ↓\n",
    "Pooled Output (768 dimensions)\n",
    "    ↓\n",
    "Dropout (p=0.5)\n",
    "    ↓\n",
    "Linear Layer (768 → 6)\n",
    "    ↓\n",
    "ReLU Activation\n",
    "    ↓\n",
    "Output Logits (6 classes)\n",
    "```\n",
    "\n",
    "### Training Configuration\n",
    "- **Batch Size**: 4\n",
    "- **Learning Rate**: 5e-7 (very small for fine-tuning)\n",
    "- **Epochs**: 20\n",
    "- **Optimizer**: Adam\n",
    "- **Loss**: CrossEntropyLoss\n",
    "- **Best Model Selection**: Based on validation F1 score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
